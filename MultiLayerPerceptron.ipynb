{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.ucd.ie/t4media/ucdmaincore_logo-footer.png)\n",
    "\n",
    "#### COMP30320 - Connectionist Computing\n",
    "\n",
    "#### Student Name: Michael Jordan\n",
    "\n",
    "#### Student Number: 14376516\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, NI, NH, NO, activation='tanh', max_epochs=5000, learning_rate=0.1, verbose=2): #activation=(tanh, linear)\n",
    "        \"\"\" \n",
    "        Initialise network.\n",
    "        NI: Number of inputs\n",
    "        NH: Number of hidden units\n",
    "        NO: Number of outputs\n",
    "        activation: Activation function\n",
    "        max_epochs: Maximum number of iterations for training\n",
    "        learning_rate: Delta applied to weights\n",
    "        verbose: Level of detail printed\n",
    "        \"\"\"\n",
    "        self.NI = NI\n",
    "        self.NH = NH\n",
    "        self.NO = NO\n",
    "        self.activation, self.d_activation = self.__init_activation(activation) \n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = np.power(10, verbose-1) \n",
    "        \n",
    "        self.loss_function = self.__init_loss_function()\n",
    "            \n",
    "        #Initialise value of units with ones. Will hold activation results in future.\n",
    "        self.I = np.ones(self.NI + 1) #Extra 1 for bias node.\n",
    "        self.H = np.ones(self.NH)\n",
    "        self.O = np.ones(self.NO)\n",
    "        \n",
    "        self.W1, self.W2, self.dW1, self.dW2 = self.randomise()\n",
    "        \n",
    "    def __init_activation(self, activation):\n",
    "        if activation == \"tanh\":\n",
    "            return tanh, d_tanh\n",
    "        if activation.lower() == \"relu\":\n",
    "            return ReLU, d_ReLU\n",
    "#         return tanh, d_tanh\n",
    "\n",
    "    def __init_loss_function(self):\n",
    "        \"\"\" \n",
    "        Initialize the loss function.\n",
    "        If multiple number of outputs, use cross entropy.\n",
    "        If a single output, use squared error.\n",
    "        \"\"\"\n",
    "        if self.NO > 1:\n",
    "            return cross_entropy\n",
    "        if self.NO == 1:\n",
    "            return squared_error\n",
    "        else:\n",
    "            print(\"Edge case: 49587323\")\n",
    "            \n",
    "    def randomise(self):\n",
    "        \"\"\" \n",
    "        Randomly initialize weights between -1 and +1 \n",
    "        for lower layer and upper layer.\n",
    "        \"\"\"\n",
    "        #Lower layer (between input and hidden layer)\n",
    "        W1 = np.random.uniform(-0.5, 0.5, (self.I.size, self.H.size)) #Values between -1 and 1 were too varied. Output inconclusive.\n",
    "        dW1 = np.zeros((self.I.size, self.H.size))\n",
    "        #Upper layer (between hidden and output layer)\n",
    "        W2 = np.random.uniform(-0.5, 0.5, (self.H.size, self.O.size))\n",
    "        dW2 = np.zeros((self.H.size, self.O.size))\n",
    "        return W1, W2, dW1, dW2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" \n",
    "        Propagate the inputs forward \n",
    "        from the input layer to the output layer\n",
    "        \"\"\"\n",
    "        #Load training inputs into the form of our environment\n",
    "        self.I[:-1] = inputs #-1 because last one is a bias node, and has already been initialised to 1 in __init__\n",
    "        \n",
    "        #Activate hidden layer\n",
    "        self.H = self.activation(np.dot(self.I, self.W1))\n",
    "        \n",
    "        #Activate output layer\n",
    "        if self.NO > 1:\n",
    "            # Classification - using SoftMax\n",
    "            self.O = softmax(np.dot(self.H, self.W2))\n",
    "        else:\n",
    "            # Regression\n",
    "            self.O = self.activation(np.dot(self.H, self.W2))\n",
    "#         self.O = softmax(np.dot(self.H, self.W2)) #Want to keep tanh/relu away from the output layer. It's best to restrict it to hidden layer.\n",
    "        \n",
    "        return self.O\n",
    "    \n",
    "    def backward(self, expected):\n",
    "        # Error on the output layer.\n",
    "        error = expected - self.O\n",
    "        \n",
    "        # Delta activation of output layer.\n",
    "        if self.NO > 1:\n",
    "            # Classification\n",
    "            dW2 = error * d_softmax(self.O, softmax)\n",
    "        else:\n",
    "            # Regression\n",
    "            dW2 = error * self.d_activation(self.O)\n",
    "        \n",
    "        # Delta activation of hidden layer.\n",
    "        dW1 = np.dot(dW2, self.W2.T) * self.d_activation(self.H) #Restricting tanh/relu to the hidden layer again.\n",
    "\n",
    "        # Update the weights\n",
    "        self.update_weights(dW1, dW2)\n",
    "        \n",
    "    def update_weights(self, dW1, dW2):\n",
    "        \"\"\" \n",
    "        Update weights on the lower layer and the upper layer.\n",
    "        \"\"\"\n",
    "        #Lower Layer\n",
    "        dW1 = np.dot(np.atleast_2d(self.I).T, np.atleast_2d(dW1))\n",
    "        self.W1 += self.learning_rate * dW1\n",
    "        \n",
    "        #Upper Layer\n",
    "        dW2 = np.dot(np.atleast_2d(self.H).T, np.atleast_2d(dW2))\n",
    "        self.W2 += self.learning_rate * dW2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\" \n",
    "        Training the MLP\n",
    "        X: The features of the training set\n",
    "        Y: The labels/Outputs of the training set\n",
    "        \"\"\"\n",
    "        for e in range(1, self.max_epochs):\n",
    "            cost = 0.0\n",
    "            for j, row in enumerate(X):\n",
    "                #Feedforward inputs to the output layer.\n",
    "                O = self.forward(row)\n",
    "                \n",
    "                #Sum the error of each example computed.\n",
    "                cost += self.loss_function(O, Y[j])\n",
    "                \n",
    "                # Backpropagate the error signal.\n",
    "                self.backward(Y[j])\n",
    "            \n",
    "            #Print details during training.\n",
    "            # Classification\n",
    "            if self.NO > 1:\n",
    "                prediction = self.predict(X) \n",
    "                accuracy = 0.0\n",
    "                for k, row in enumerate(Y):\n",
    "                    if prediction[k] == np.argmax(Y[k]):\n",
    "                        accuracy += 1\n",
    "                        \n",
    "                if e % self.verbose == 0:\n",
    "                    print('Epoch: %d \\t|\\t Error: %.6f \\t|\\t Accuracy: %.3f' %(e, cost/len(X), accuracy/len(X)))\n",
    "            \n",
    "            # Regression\n",
    "            else:\n",
    "                if e % self.verbose == 0:\n",
    "                    print('Epoch: %d \\t|\\t Error: %.6f' % (e, cost/(len(X))))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Predict on the test set\n",
    "        X: Unknown features of the test set\n",
    "        \"\"\"\n",
    "        Y = list()\n",
    "        for j, row in enumerate(X):\n",
    "            if self.NO > 1:\n",
    "                # Classification - using one hot encoding,\n",
    "                #   so find the index of output units with the max output\n",
    "                Y.append(np.argmax(self.forward(row)))\n",
    "            else:\n",
    "                Y.append(self.forward(row))\n",
    "        \n",
    "        return np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of activation functions and their derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "# def ReLU(x):\n",
    "#     if x[0] < 0:\n",
    "#         x[0] = 0\n",
    "#     if x[1] < 0:\n",
    "#         x[1] = 0\n",
    "#     return x\n",
    "\n",
    "# def d_ReLU(x):\n",
    "#     if x[0] < 0:\n",
    "#         x[0] = 0\n",
    "# #     if x[1] < 0:\n",
    "# #         x[1] = 0\n",
    "#     return x\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: input that needs to be activated\n",
    "    return: softmax on x\n",
    "    \"\"\"\n",
    "    x_exp = [math.exp(i) for i in x]\n",
    "    sum_x_exp = sum(x_exp)\n",
    "    \n",
    "    if sum_x_exp != 0:\n",
    "        return [i / sum_x_exp for i in x_exp]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def d_softmax(output, function):\n",
    "    \"\"\" \n",
    "    output: Output layer\n",
    "    function: The activation function of the output layer - SoftMax in classification\n",
    "    return: derivative of softmax\n",
    "    \"\"\"\n",
    "    return function(output) * (1 - function(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(o, y):\n",
    "    return np.sum(np.nan_to_num(-y * np.log(o) - (1-y) * np.log(1-o)))\n",
    "\n",
    "\n",
    "def squared_error(o, y):\n",
    "    return 0.5 * ((y-o) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an MLP with 2 inputs, two hidden units and one output on the following examples (XOR function):\n",
    "   ((0, 0), 0)\n",
    "   \n",
    "   ((0, 1), 1)\n",
    "   \n",
    "   ((1, 0), 1)\n",
    "   \n",
    "   ((1, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \t|\t Error: 0.132236\n",
      "Epoch: 200 \t|\t Error: 0.131917\n",
      "Epoch: 300 \t|\t Error: 0.131774\n",
      "Epoch: 400 \t|\t Error: 0.131724\n",
      "Epoch: 500 \t|\t Error: 0.131716\n",
      "Epoch: 600 \t|\t Error: 0.131709\n",
      "Epoch: 700 \t|\t Error: 0.131612\n",
      "Epoch: 800 \t|\t Error: 0.131103\n",
      "Epoch: 900 \t|\t Error: 0.128999\n",
      "Epoch: 1000 \t|\t Error: 0.120203\n",
      "Epoch: 1100 \t|\t Error: 0.082431\n",
      "Epoch: 1200 \t|\t Error: 0.053369\n",
      "Epoch: 1300 \t|\t Error: 0.037276\n",
      "Epoch: 1400 \t|\t Error: 0.028075\n",
      "Epoch: 1500 \t|\t Error: 0.022301\n",
      "Epoch: 1600 \t|\t Error: 0.018391\n",
      "Epoch: 1700 \t|\t Error: 0.015588\n",
      "Epoch: 1800 \t|\t Error: 0.013490\n",
      "Epoch: 1900 \t|\t Error: 0.011868\n",
      "-----------------------------------------------------------------\n",
      "Input: [0 0] \t|\t Expected: 0 \t|\t Output: 0\n",
      "Input: [0 1] \t|\t Expected: 1 \t|\t Output: 1\n",
      "Input: [1 0] \t|\t Expected: 1 \t|\t Output: 1\n",
      "Input: [1 1] \t|\t Expected: 0 \t|\t Output: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XOR inputs\n",
    "XOR_inputs = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0]\n",
    "])\n",
    "\n",
    "# Split the inputs and outputs\n",
    "x = XOR_inputs[:, :-1]\n",
    "y = XOR_inputs[:, -1]\n",
    "\n",
    "# Initialize the MLP network\n",
    "mlp = MLP(2, 2, 1, max_epochs=2000, verbose=3)\n",
    "\n",
    "# Training the MLP\n",
    "mlp.fit(x, y)\n",
    "\n",
    "# Predict the MLP on XOR inputs\n",
    "prediction = mlp.predict(x)\n",
    "print(\"-----\" * 13)\n",
    "for i, l in enumerate(y):\n",
    "    print('Input: %s \\t|\\t Expected: %.f \\t|\\t Output: %.0f' % (str(x[i]), y[i], prediction[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the end of training, check if the MLP predicts correctly all the examples.\n",
    "   Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Test 1\n",
    "Here, we trained a model to identify the XOR solution. This is a good problem for neural networks as it's something that can't be solved with a single linear function. \n",
    "\n",
    "I think it ran very well. Ending with an error of only 0.011868, and predicting the outputs correctly, I can deem this a success. \n",
    "\n",
    "One thing to note, I found altering the weights (W1 and W2) to be between -0.5 and +0.5 to be much more efficient and conclusive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sin function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 200 vectors containing 4 components each. \n",
    "#### The value of each component should be a random number between -1 and 1. \n",
    "#### These will be your input vectors. \n",
    "#### The corresponding output for each vector should be the sin() of a combination of the components.\n",
    " \n",
    " Example:\n",
    " \n",
    "    Input:  [x1 x2 x3 x4]\n",
    " \n",
    "    Output: sin(x1-x2+x3-x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP network\n",
    "mlp = MLP(4, 10, 1, learning_rate=0.1, max_epochs=3000, verbose=3)\n",
    "\n",
    "# Initialize the training set and test set\n",
    "sample_num = 200\n",
    "train_num = 150\n",
    "test_num = 50\n",
    "\n",
    "# Inputs\n",
    "X = np.random.uniform(-1, 1, (sample_num, 4))\n",
    "\n",
    "# Output\n",
    "Y = list(map(lambda a: np.sin(a[0] - a[1] + a[2] - a[3]), X))\n",
    "\n",
    "# Combine input and output\n",
    "ds = np.column_stack((X, Y))\n",
    "\n",
    "# Split the training set and test set\n",
    "train = ds[:train_num]\n",
    "test = ds[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train an MLP with 4 inputs, at least 5 hidden units and one output on 150 of these examples and keep the remaining 50 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \t|\t Error: 0.008666\n",
      "Epoch: 200 \t|\t Error: 0.008743\n",
      "Epoch: 300 \t|\t Error: 0.007680\n",
      "Epoch: 400 \t|\t Error: 0.006302\n",
      "Epoch: 500 \t|\t Error: 0.006097\n",
      "Epoch: 600 \t|\t Error: 0.005568\n",
      "Epoch: 700 \t|\t Error: 0.004433\n",
      "Epoch: 800 \t|\t Error: 0.001439\n",
      "Epoch: 900 \t|\t Error: 0.000521\n",
      "Epoch: 1000 \t|\t Error: 0.000357\n",
      "Epoch: 1100 \t|\t Error: 0.000379\n",
      "Epoch: 1200 \t|\t Error: 0.000444\n",
      "Epoch: 1300 \t|\t Error: 0.000545\n",
      "Epoch: 1400 \t|\t Error: 0.000677\n",
      "Epoch: 1500 \t|\t Error: 0.000755\n",
      "Epoch: 1600 \t|\t Error: 0.000730\n",
      "Epoch: 1700 \t|\t Error: 0.000696\n",
      "Epoch: 1800 \t|\t Error: 0.000692\n",
      "Epoch: 1900 \t|\t Error: 0.000700\n",
      "Epoch: 2000 \t|\t Error: 0.000691\n",
      "Epoch: 2100 \t|\t Error: 0.000608\n",
      "Epoch: 2200 \t|\t Error: 0.000433\n",
      "Epoch: 2300 \t|\t Error: 0.000315\n",
      "Epoch: 2400 \t|\t Error: 0.000236\n",
      "Epoch: 2500 \t|\t Error: 0.000180\n",
      "Epoch: 2600 \t|\t Error: 0.000158\n",
      "Epoch: 2700 \t|\t Error: 0.000139\n",
      "Epoch: 2800 \t|\t Error: 0.000113\n",
      "Epoch: 2900 \t|\t Error: 0.000095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x117575f98>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the MLP network\n",
    "mlp.fit(train[:, :-1], train[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74742818 -0.52278696  0.1004962   0.36804979] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[ 0.95390476 -0.48334988  0.08922534  0.68245714] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[0.49959348 0.55808257 0.82440938 0.58854471] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[ 0.05079309 -0.18673182 -0.22610956  0.35493546] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.57611149 -0.30889683  0.73289491 -0.80030568] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.28978418  0.52940615 -0.8974755  -0.43615098] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.88746991  0.71273133 -0.05571403  0.63652154] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[ 0.36789789 -0.35591926  0.67324562 -0.15543255] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[ 0.4163947   0.59717439  0.90459803 -0.47961734] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[ 0.72685379 -0.74594506  0.74759361 -0.61553623] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.20072192  0.71258938  0.2548554   0.21476807] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.93481544 -0.21645127  0.53328865  0.90192025] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[ 0.60966682  0.12865448  0.44853218 -0.22879085] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.92838221 -0.84549854 -0.3197981  -0.82368473] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[ 0.96780578  0.44405067 -0.49523929  0.63726024] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.97363573 -0.34803085 -0.2216496   0.70961036] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.91273053  0.19441807  0.57794272 -0.16016097] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[0.21709739 0.30097058 0.46763824 0.89930432] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.02046175 -0.64791235  0.83336021  0.31466681] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.793595    0.72406627  0.63760288 -0.60481008] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[ 0.19548796 -0.65268012 -0.99630184  0.87197135] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.29020665  0.38955249  0.1758893   0.10048644] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.44592867  0.75643605  0.49036644  0.06997359] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.65521366 -0.32855754  0.35344217  0.20459768] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[ 0.85772552  0.3915717  -0.31288341 -0.74509487] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.19243605 -0.72984335 -0.42515161 -0.03416387] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[ 0.97674699 -0.71237586 -0.59214011  0.99226481] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.19050838  0.42322574  0.66316293 -0.57802764] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.85811249 -0.82572178  0.19961687  0.12757518] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[ 0.90571073 -0.68169325 -0.36159541 -0.12601178] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[ 0.79697769  0.99074885 -0.29920614 -0.77280383] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.80255175 -0.24116643  0.4765894  -0.0363357 ] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.06153126 -0.92873881 -0.11376753 -0.0742488 ] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.80371492 -0.73237062  0.9089755   0.90105756] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.80207291  0.88282283 -0.017836    0.65612328] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[-0.74270105 -0.77411224  0.73904384  0.3513538 ] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.01677645 -0.36535888 -0.70563994  0.90242565] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[ 0.65596665 -0.39374698 -0.84796877  0.68949716] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.17054142 -0.89128722 -0.42799039 -0.21102667] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.83029776 -0.41409999  0.90509151  0.22499311] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[ 0.67254474 -0.42222531 -0.30337218 -0.41564626] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.30645284  0.49699431  0.78618639 -0.42729304] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[0.59710419 0.30792477 0.47331191 0.86686799] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[-0.38487257 -0.01498698  0.5599974  -0.71908469] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[-0.47955074  0.68293029  0.99111848 -0.28385953] \t|\t Expected: 0 \t|\t Output: 0\n",
      "[-0.46519668 -0.81697715  0.10171431  0.93333794] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[ 0.81186042  0.48873342 -0.95497213 -0.26620495] \t|\t Expected: -0 \t|\t Output: -0\n",
      "[ 0.36774879  0.5803599  -0.02074128  0.61356982] \t|\t Expected: -1 \t|\t Output: -1\n",
      "[ 2.28947270e-04 -4.64093949e-01 -2.45556227e-01 -9.29245740e-01] \t|\t Expected: 1 \t|\t Output: 1\n",
      "[ 0.81514413 -0.63864946  0.34675191  0.23458459] \t|\t Expected: 1 \t|\t Output: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Error on test set: 0.00033\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "test_x = test[:, :-1]\n",
    "test_y = test[:, -1]\n",
    "\n",
    "prediction = mlp.predict(test_x).flatten()\n",
    "\n",
    "cost = 0.0\n",
    "accuracy = 0\n",
    "total_i = 0\n",
    "for i, k in enumerate(test_x):\n",
    "    cost += 0.5 * (test_y[i] - prediction[i]) ** 2\n",
    "    print('%s \\t|\\t Expected: %.f \\t|\\t Output: %.0f' % (str(k), test_y[i], prediction[i]))\n",
    "    \n",
    "    if (int(test_y[i]) == int(prediction[i])):\n",
    "        accuracy += 1\n",
    "    total_i += 1\n",
    "        \n",
    "print('-----' * 20)\n",
    "print('Error on test set: %.5f' % (cost / len(test_x)))\n",
    "print('Accuracy: %.2f' %(accuracy/total_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the error on training at the end? How does it compare with the error on the test set? Do you think you have learned satisfactorily?\n",
    "\n",
    "Error at end of training is 0.000129, compared to the error at end of testing 0.00026.\n",
    "\n",
    "While all small errors, you could say there is a significant difference, as error during training is half that of testing.\n",
    "\n",
    "This is as expected, as there are bound to be more errors when predicting on unseen  data.\n",
    "\n",
    "I believe the learning has been satisfactory. The accuracy on the test set came out to 100%. Couldn't have gone better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Test 2\n",
    "These outcomes proved that our MLP is capable of solving for quite complex functions. \n",
    "\n",
    "Here we solved for the sin of a combination of four attributes. \n",
    "\n",
    "I was surprised by how efficient it was with a sample size of only 200 (very small in my opnion). Impressive performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
